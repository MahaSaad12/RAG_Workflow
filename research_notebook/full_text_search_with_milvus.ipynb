{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b982406",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/milvus-io/bootcamp/blob/master/bootcamp/tutorials/quickstart/full_text_search_with_milvus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>   <a href=\"https://github.com/milvus-io/bootcamp/blob/master/bootcamp/tutorials/quickstart/full_text_search_with_milvus.ipynb\" target=\"_blank\">\n",
    "    <img src=\"https://img.shields.io/badge/View%20on%20GitHub-555555?style=flat&logo=github&logoColor=white\" alt=\"GitHub Repository\"/>\n",
    "</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32257120-dd18-430b-9d80-14bb75bc7d8b",
   "metadata": {},
   "source": [
    "# Hybrid Retrieval with Full-Text Search\n",
    "\n",
    "[Full-text search](https://milvus.io/docs/full-text-search.md#Full-Text-Search) is a traditional method for retrieving documents by matching specific keywords or phrases in the text. It ranks results based on relevance scores calculated from factors like term frequency. While semantic search is better at understanding meaning and context, full-text search excels at precise keyword matching, making it a useful complement to semantic search. A common approach to constructing a Retrieval-Augmented Generation (RAG) pipeline involves retrieving documents through both semantic search and full-text search, followed by a reranking process to refine the results.\n",
    "\n",
    "![](../../../images/advanced_rag/hybrid_and_rerank.png)\n",
    "\n",
    "This approach converts text into sparse vectors for BM25 scoring. To ingest documents, users can simply input raw text without computing the sparse vector manually. Milvus will automatically generate and store the sparse vectors. To search documents, users just need to specify the text search query. Milvus will compute BM25 scores internally and return ranked results.\n",
    "\n",
    "\n",
    "Milvus also supports hybrid retrieval by combining full-text search with dense vector based semantic search. It usually improves search quality and delivers better results to users by balancing keyword matching and semantic understanding.\n",
    "\n",
    "> - Full-text search is currently available in Milvus Standalone, Milvus Distributed, and Zilliz Cloud, though not yet supported in Milvus Lite (which has this feature planned for future implementation). Reach out support@zilliz.com for more information.\n",
    "\n",
    "\n",
    "## Preparation\n",
    "\n",
    "### Install PyMilvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "779da698-4ce5-4555-8f0b-11dc90408525",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pymilvus -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6856cf93-e297-40a1-83f2-e71bc99a3ca9",
   "metadata": {},
   "source": [
    "> If you are using Google Colab, to enable dependencies just installed, you may need to **restart the runtime** (click on the \"Runtime\" menu at the top of the screen, and select \"Restart session\" from the dropdown menu).\n",
    "\n",
    "### Set OpenAI API Key\n",
    "We will use the models from OpenAI for creating vector embeddings and generation response. You should prepare the [api key](https://platform.openai.com/docs/quickstart) `OPENAI_API_KEY` as an environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c95ab9e1-a221-4840-bf0d-b1801eaa14ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maha_\\RAG_Workflow\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Append the parent folder, not 'src'\n",
    "sys.path.append(os.path.abspath(r'C:\\Users\\maha_\\RAG_Workflow'))\n",
    "\n",
    "from src.embedding_model.multilingual_embed import MultilingualEmbeddingModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352e32be-2085-4c88-a05a-cce7303cf254",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c24491e3-1334-4ce5-92a8-1aa13e73a137",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "from pymilvus import (\n",
    "    MilvusClient,\n",
    "    DataType,\n",
    "    Function,\n",
    "    FunctionType,\n",
    "    AnnSearchRequest,\n",
    "    RRFRanker,\n",
    ")\n",
    "\n",
    "from src.embedding_model.multilingual_embed import MultilingualEmbeddingModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60c238f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class HybridRetriever:\n",
    "    def __init__(self, uri, db_name, collection_name=\"hybrid\", \n",
    "                 dense_embedding_function=None):\n",
    "        self.uri = uri\n",
    "        self.db_name = db_name\n",
    "        self.collection_name = collection_name\n",
    "        self.embedding_function = dense_embedding_function\n",
    "        self.use_reranker = True\n",
    "        self.use_sparse = True\n",
    "        self.client = MilvusClient(uri=self.uri, db_name=self.db_name)\n",
    "\n",
    "    def build_collection(self):\n",
    "        # if isinstance(self.embedding_function.dim, dict):\n",
    "        #     dense_dim = self.embedding_function.dim[\"dense\"]\n",
    "        # else:\n",
    "        dense_dim = 512\n",
    "\n",
    "        tokenizer_params = {\n",
    "            \"tokenizer\": \"standard\",\n",
    "            \"filter\": [\n",
    "                \"lowercase\",\n",
    "                {\n",
    "                    \"type\": \"length\",\n",
    "                    \"max\": 200,\n",
    "                },\n",
    "                {\"type\": \"stemmer\", \"language\": \"english\"},\n",
    "                {\n",
    "                    \"type\": \"stop\",\n",
    "                    \"stop_words\": [\n",
    "                        \"a\",\n",
    "                        \"an\",\n",
    "                        \"and\",\n",
    "                        \"are\",\n",
    "                        \"as\",\n",
    "                        \"at\",\n",
    "                        \"be\",\n",
    "                        \"but\",\n",
    "                        \"by\",\n",
    "                        \"for\",\n",
    "                        \"if\",\n",
    "                        \"in\",\n",
    "                        \"into\",\n",
    "                        \"is\",\n",
    "                        \"it\",\n",
    "                        \"no\",\n",
    "                        \"not\",\n",
    "                        \"of\",\n",
    "                        \"on\",\n",
    "                        \"or\",\n",
    "                        \"such\",\n",
    "                        \"that\",\n",
    "                        \"the\",\n",
    "                        \"their\",\n",
    "                        \"then\",\n",
    "                        \"there\",\n",
    "                        \"these\",\n",
    "                        \"they\",\n",
    "                        \"this\",\n",
    "                        \"to\",\n",
    "                        \"was\",\n",
    "                        \"will\",\n",
    "                        \"with\",\n",
    "                    ],\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "\n",
    "        schema = MilvusClient.create_schema(enable_dynamic_field=True)\n",
    "        schema.add_field(\n",
    "            field_name=\"pk\",\n",
    "            datatype=DataType.VARCHAR,\n",
    "            is_primary=True,\n",
    "            auto_id=True,\n",
    "            max_length=100,\n",
    "        )\n",
    "        schema.add_field(\n",
    "            field_name=\"content\",\n",
    "            datatype=DataType.VARCHAR,\n",
    "            max_length=65535,\n",
    "            analyzer_params=tokenizer_params,\n",
    "            enable_match=True,\n",
    "            enable_analyzer=True,\n",
    "        )\n",
    "        schema.add_field(\n",
    "            field_name=\"sparse_vector\", datatype=DataType.SPARSE_FLOAT_VECTOR\n",
    "        )\n",
    "        schema.add_field(\n",
    "            field_name=\"dense_vector\", datatype=DataType.FLOAT_VECTOR, dim=dense_dim\n",
    "        )\n",
    "        schema.add_field(\n",
    "            field_name=\"original_uuid\", datatype=DataType.VARCHAR, max_length=128\n",
    "        )\n",
    "        schema.add_field(field_name=\"doc_id\", datatype=DataType.VARCHAR, max_length=64)\n",
    "        schema.add_field(\n",
    "            field_name=\"chunk_id\", datatype=DataType.VARCHAR, max_length=64\n",
    "        ),\n",
    "        schema.add_field(field_name=\"original_index\", datatype=DataType.INT32)\n",
    "\n",
    "        functions = Function(\n",
    "            name=\"bm25\",\n",
    "            function_type=FunctionType.BM25,\n",
    "            input_field_names=[\"content\"],\n",
    "            output_field_names=[\"sparse_vector\"],\n",
    "        )\n",
    "\n",
    "        schema.add_function(functions)\n",
    "\n",
    "        index_params = MilvusClient.prepare_index_params()\n",
    "        index_params.add_index(\n",
    "            field_name=\"sparse_vector\",\n",
    "            index_type=\"SPARSE_INVERTED_INDEX\",\n",
    "            metric_type=\"BM25\",\n",
    "        )\n",
    "        index_params.add_index(\n",
    "            field_name=\"dense_vector\", index_type=\"AUTOINDEX\", metric_type=\"COSINE\"\n",
    "        )\n",
    "\n",
    "        self.client.create_collection(\n",
    "            collection_name=self.collection_name,\n",
    "            schema=schema,\n",
    "            index_params=index_params,\n",
    "        )\n",
    "\n",
    "    def insert_data(self, chunk, metadata):\n",
    "        embedding = self.embedding_function.embed([chunk])[0]\n",
    "        if isinstance(embedding, dict) and \"dense\" in embedding:\n",
    "            dense_vec = embedding[\"dense\"][0]\n",
    "        else:\n",
    "            dense_vec = embedding\n",
    "        self.client.insert(\n",
    "            self.collection_name, {\"dense_vector\": dense_vec, **metadata}\n",
    "        )\n",
    "\n",
    "    def search(self, query: str, k: int = 20, mode=\"hybrid\"):\n",
    "\n",
    "        output_fields = [\n",
    "            \"content\",\n",
    "            \"original_uuid\",\n",
    "            \"doc_id\",\n",
    "            \"chunk_id\",\n",
    "            \"original_index\",\n",
    "        ]\n",
    "        if mode in [\"dense\", \"hybrid\"]:\n",
    "            embedding = self.embedding_function.embed([query])\n",
    "            if isinstance(embedding, dict) and \"dense\" in embedding:\n",
    "                dense_vec = embedding[\"dense\"][0]\n",
    "            else:\n",
    "                dense_vec = embedding[0]\n",
    "\n",
    "        if mode == \"sparse\":\n",
    "            results = self.client.search(\n",
    "                collection_name=self.collection_name,\n",
    "                data=[query],\n",
    "                anns_field=\"sparse_vector\",\n",
    "                limit=k,\n",
    "                output_fields=output_fields,\n",
    "            )\n",
    "        elif mode == \"dense\":\n",
    "            results = self.client.search(\n",
    "                collection_name=self.collection_name,\n",
    "                data=[dense_vec],\n",
    "                anns_field=\"dense_vector\",\n",
    "                limit=k,\n",
    "                output_fields=output_fields,\n",
    "            )\n",
    "        elif mode == \"hybrid\":\n",
    "            full_text_search_params = {\"metric_type\": \"BM25\"}\n",
    "            full_text_search_req = AnnSearchRequest(\n",
    "                [query], \"sparse_vector\", full_text_search_params, limit=k\n",
    "            )\n",
    "\n",
    "            dense_search_params = {\"metric_type\": \"COSINE\"}\n",
    "            dense_req = AnnSearchRequest(\n",
    "                [dense_vec], \"dense_vector\", dense_search_params, limit=k\n",
    "            )\n",
    "\n",
    "            results = self.client.hybrid_search(\n",
    "                self.collection_name,\n",
    "                [full_text_search_req, dense_req],\n",
    "                ranker=RRFRanker(),\n",
    "                limit=k,\n",
    "                output_fields=output_fields,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mode\")\n",
    "        return [\n",
    "            {\n",
    "                \"doc_id\": doc[\"entity\"][\"doc_id\"],\n",
    "                \"chunk_id\": doc[\"entity\"][\"chunk_id\"],\n",
    "                \"content\": doc[\"entity\"][\"content\"],\n",
    "                \"score\": doc[\"distance\"],\n",
    "            }\n",
    "            for doc in results[0]\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d7033a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultilingualEmbeddingModel()\n",
    "model.load_model()\n",
    "standard_retriever = HybridRetriever(\n",
    "    uri=\"http://localhost:19530\",\n",
    "    db_name=\"default\",\n",
    "    collection_name=\"milvus_hybrid\",\n",
    "    dense_embedding_function=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229d49f5-4721-425f-8848-ed2b3bb8be57",
   "metadata": {},
   "source": [
    "We'll use the MilvusClient to establish a connection to the Milvus server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbc5f55",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13cb896a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Documents: 100%|██████████| 90/90 [01:30<00:00,  1.01s/doc]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "path = \"codebase_chunks.json\"\n",
    "with open(path, \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "is_insert = True\n",
    "if is_insert:\n",
    "    standard_retriever.build_collection()\n",
    "    \n",
    "    # Wrap the dataset with tqdm for progress tracking\n",
    "    for doc in tqdm(dataset, desc=\"Processing Documents\", unit=\"doc\"):\n",
    "        doc_content = doc[\"content\"]\n",
    "        \n",
    "        # Wrap the chunks of each document with tqdm\n",
    "        for chunk in tqdm(doc[\"chunks\"], desc=\"Processing Chunks\", unit=\"chunk\", leave=False):\n",
    "            metadata = {\n",
    "                \"doc_id\": doc[\"doc_id\"],\n",
    "                \"original_uuid\": doc[\"original_uuid\"],\n",
    "                \"chunk_id\": chunk[\"chunk_id\"],\n",
    "                \"original_index\": chunk[\"original_index\"],\n",
    "                \"content\": chunk[\"content\"],\n",
    "            }\n",
    "            chunk_content = chunk[\"content\"]\n",
    "            \n",
    "            # Insert data with progress tracking\n",
    "            standard_retriever.insert_data(chunk_content[:500], metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81f42e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'doc_id': 'doc_1', 'chunk_id': 'doc_1_chunk_0', 'content': '//! Executor for differential fuzzing.\\n//! It wraps two executors that will be run after each other with the same input.\\n//! In comparison to the [`crate::executors::CombinedExecutor`] it also runs the secondary executor in `run_target`.\\n//!\\nuse core::{cell::UnsafeCell, fmt::Debug, ptr};\\n\\nuse libafl_bolts::{ownedref::OwnedMutPtr, tuples::MatchName};\\nuse serde::{Deserialize, Serialize};\\n\\nuse crate::{\\n    executors::{Executor, ExitKind, HasObservers},\\n    inputs::UsesInput,\\n    observers::{DifferentialObserversTuple, ObserversTuple, UsesObservers},\\n    state::UsesState,\\n    Error,\\n};\\n\\n/// A [`DiffExecutor`] wraps a primary executor, forwarding its methods, and a secondary one\\n#[derive(Debug)]\\npub struct DiffExecutor<A, B, OTA, OTB, DOT> {\\n    primary: A,\\n    secondary: B,\\n    observers: UnsafeCell<ProxyObserversTuple<OTA, OTB, DOT>>,\\n}\\n\\n', 'score': 0.016393441706895828}, {'doc_id': 'doc_63', 'chunk_id': 'doc_63_chunk_9', 'content': '    // Setup child stdin/stdout/stderr as slave fd of PTY.\\n    // Ownership of fd is transferred to the Stdio structs and will be closed by them at the end of\\n    // this scope. (It is not an issue that the fd is closed three times since File::drop ignores\\n    // error on libc::close.).\\n    builder.stdin(unsafe { Stdio::from_raw_fd(slave_fd) });\\n    builder.stderr(unsafe { Stdio::from_raw_fd(slave_fd) });\\n    builder.stdout(unsafe { Stdio::from_raw_fd(slave_fd) });\\n\\n', 'score': 0.016393441706895828}, {'doc_id': 'doc_1', 'chunk_id': 'doc_1_chunk_2', 'content': '    /// Retrieve the primary `Executor` that is wrapped by this `DiffExecutor`.\\n    pub fn primary(&mut self) -> &mut A {\\n        &mut self.primary\\n    }\\n\\n    /// Retrieve the secondary `Executor` that is wrapped by this `DiffExecutor`.\\n    pub fn secondary(&mut self) -> &mut B {\\n        &mut self.secondary\\n    }\\n}\\n\\nimpl<A, B, EM, DOT, Z> Executor<EM, Z> for DiffExecutor<A, B, A::Observers, B::Observers, DOT>\\nwhere\\n    A: Executor<EM, Z> + HasObservers,\\n    B: Executor<EM, Z, State = A::State> + HasObservers,\\n    EM: UsesState<State = A::State>,\\n    DOT: DifferentialObserversTuple<A::Observers, B::Observers, A::State>,\\n    Z: UsesState<State = A::State>,\\n{\\n    fn run_target(\\n        &mut self,\\n        fuzzer: &mut Z,\\n        state: &mut Self::State,\\n        mgr: &mut EM,\\n        input: &Self::Input,\\n    ) -> Result<ExitKind, Error> {\\n        self.observers(); // update in advance\\n        let observers = self.observers.get_mut();\\n        observers\\n            .differential\\n', 'score': 0.016129031777381897}]\n"
     ]
    }
   ],
   "source": [
    "results = standard_retriever.search(\"What is the purpose of the DiffExecutor struct?\", mode=\"hybrid\", k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8c3e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d99210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\maha_\\rag_workflow\\.venv\\lib\\site-packages (1.82.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\maha_\\rag_workflow\\.venv\\lib\\site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\maha_\\rag_workflow\\.venv\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\maha_\\rag_workflow\\.venv\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\maha_\\rag_workflow\\.venv\\lib\\site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\maha_\\rag_workflow\\.venv\\lib\\site-packages (from openai) (2.11.5)\n",
      "Requirement already satisfied: sniffio in c:\\users\\maha_\\rag_workflow\\.venv\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\maha_\\rag_workflow\\.venv\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\maha_\\rag_workflow\\.venv\\lib\\site-packages (from openai) (4.13.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\maha_\\rag_workflow\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\maha_\\rag_workflow\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\maha_\\rag_workflow\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\maha_\\rag_workflow\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\maha_\\rag_workflow\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\maha_\\rag_workflow\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\maha_\\rag_workflow\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\maha_\\rag_workflow\\.venv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cc8804",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d0db943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "class OpenAILLM:\n",
    "    def __init__(self, api_key: str, model: str = \"gpt-4\"):\n",
    "        self.api_key = api_key\n",
    "        self.model = model\n",
    "        openai.api_key = self.api_key\n",
    "\n",
    "    def generate(self, prompt: str, temperature: float = 0.7, max_tokens: int = 512) -> str:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        return response['choices'][0]['message']['content'].strip()\n",
    "\n",
    "# Example usage:\n",
    "# llm = OpenAILLM(api_key=\"your-openai-api-key\")\n",
    "# print(llm.generate(\"Explain quantum computing in simple terms.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cbdf85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_with_docs(retrieved_docs: list[str], user_query: str) -> str:\n",
    "    \"\"\"\n",
    "    Build a prompt for the LLM using retrieved documents and a user query.\n",
    "\n",
    "    Args:\n",
    "        retrieved_docs (list[str]): List of strings representing the retrieved documents.\n",
    "        user_query (str): The user's original question.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted prompt to send to the LLM.\n",
    "    \"\"\"\n",
    "    context = \"\\n\\n\".join(f\"Document {i+1}:\\n{doc}\" for i, doc in enumerate(retrieved_docs))\n",
    "    prompt = (\n",
    "        f\"You are an AI assistant helping a user based on the following retrieved documents:\\n\\n\"\n",
    "        f\"{context}\\n\\n\"\n",
    "        f\"Answer the following question using only the information in the documents:\\n\"\n",
    "        f\"\\\"{user_query}\\\"\\n\\n\"\n",
    "        f\"If the answer cannot be found in the documents, say \\\"I don't know based on the provided information.\\\"\"\n",
    "    )\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c06e3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "APIRemovedInV1",
     "evalue": "\n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAPIRemovedInV1\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Use the OpenAI LLM to generate an answer\u001b[39;00m\n\u001b[32m     13\u001b[39m llm = OpenAILLM(api_key=\u001b[33m\"\u001b[39m\u001b[33myour-api-key\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m answer = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(answer)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mOpenAILLM.generate\u001b[39m\u001b[34m(self, prompt, temperature, max_tokens)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m, prompt: \u001b[38;5;28mstr\u001b[39m, temperature: \u001b[38;5;28mfloat\u001b[39m = \u001b[32m0.7\u001b[39m, max_tokens: \u001b[38;5;28mint\u001b[39m = \u001b[32m512\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     response = \u001b[43mopenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_tokens\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response[\u001b[33m'\u001b[39m\u001b[33mchoices\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m].strip()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\maha_\\RAG_Workflow\\.venv\\Lib\\site-packages\\openai\\lib\\_old_api.py:39\u001b[39m, in \u001b[36mAPIRemovedInV1Proxy.__call__\u001b[39m\u001b[34m(self, *_args, **_kwargs)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *_args: Any, **_kwargs: Any) -> Any:\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m APIRemovedInV1(symbol=\u001b[38;5;28mself\u001b[39m._symbol)\n",
      "\u001b[31mAPIRemovedInV1\u001b[39m: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
     ]
    }
   ],
   "source": [
    "\n",
    "retrieved_docs = [\n",
    "    \"Milvus is a vector database designed for high-performance similarity search.\",\n",
    "    \"It supports scalar filtering, indexing, and full-text search via BM25.\"\n",
    "]\n",
    "\n",
    "user_query = \"How does Milvus support full-text search?\"\n",
    "\n",
    "\n",
    "prompt = build_prompt_with_docs(retrieved_docs, user_query)\n",
    "\n",
    "\n",
    "llm = OpenAILLM(api_key=\"your-api-key\")\n",
    "answer = llm.generate(prompt)\n",
    "print(answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
